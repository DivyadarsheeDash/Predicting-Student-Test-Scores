{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14520824,"sourceType":"datasetVersion","datasetId":9274233},{"sourceId":14520841,"sourceType":"datasetVersion","datasetId":9274244}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-18T10:45:55.855262Z","iopub.execute_input":"2026-01-18T10:45:55.855623Z","iopub.status.idle":"2026-01-18T10:45:57.409116Z","shell.execute_reply.started":"2026-01-18T10:45:55.855584Z","shell.execute_reply":"2026-01-18T10:45:57.407913Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/playground-train/train.csv\")\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T10:45:57.411465Z","iopub.execute_input":"2026-01-18T10:45:57.411980Z","iopub.status.idle":"2026-01-18T10:45:58.748673Z","shell.execute_reply.started":"2026-01-18T10:45:57.411947Z","shell.execute_reply":"2026-01-18T10:45:58.747515Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   id  age  gender   course  study_hours  class_attendance internet_access  \\\n0   0   21  female     b.sc         7.91              98.8              no   \n1   1   18   other  diploma         4.95              94.8             yes   \n2   2   20  female     b.sc         4.68              92.6             yes   \n3   3   19    male     b.sc         2.00              49.5             yes   \n4   4   23    male      bca         7.65              86.9             yes   \n\n   sleep_hours sleep_quality   study_method facility_rating exam_difficulty  \\\n0          4.9       average  online videos             low            easy   \n1          4.7          poor     self-study          medium        moderate   \n2          5.8          poor       coaching            high        moderate   \n3          8.3       average    group study            high        moderate   \n4          9.6          good     self-study            high            easy   \n\n   exam_score  \n0        78.3  \n1        46.7  \n2        99.0  \n3        63.9  \n4       100.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>age</th>\n      <th>gender</th>\n      <th>course</th>\n      <th>study_hours</th>\n      <th>class_attendance</th>\n      <th>internet_access</th>\n      <th>sleep_hours</th>\n      <th>sleep_quality</th>\n      <th>study_method</th>\n      <th>facility_rating</th>\n      <th>exam_difficulty</th>\n      <th>exam_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>21</td>\n      <td>female</td>\n      <td>b.sc</td>\n      <td>7.91</td>\n      <td>98.8</td>\n      <td>no</td>\n      <td>4.9</td>\n      <td>average</td>\n      <td>online videos</td>\n      <td>low</td>\n      <td>easy</td>\n      <td>78.3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>18</td>\n      <td>other</td>\n      <td>diploma</td>\n      <td>4.95</td>\n      <td>94.8</td>\n      <td>yes</td>\n      <td>4.7</td>\n      <td>poor</td>\n      <td>self-study</td>\n      <td>medium</td>\n      <td>moderate</td>\n      <td>46.7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>20</td>\n      <td>female</td>\n      <td>b.sc</td>\n      <td>4.68</td>\n      <td>92.6</td>\n      <td>yes</td>\n      <td>5.8</td>\n      <td>poor</td>\n      <td>coaching</td>\n      <td>high</td>\n      <td>moderate</td>\n      <td>99.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>19</td>\n      <td>male</td>\n      <td>b.sc</td>\n      <td>2.00</td>\n      <td>49.5</td>\n      <td>yes</td>\n      <td>8.3</td>\n      <td>average</td>\n      <td>group study</td>\n      <td>high</td>\n      <td>moderate</td>\n      <td>63.9</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>23</td>\n      <td>male</td>\n      <td>bca</td>\n      <td>7.65</td>\n      <td>86.9</td>\n      <td>yes</td>\n      <td>9.6</td>\n      <td>good</td>\n      <td>self-study</td>\n      <td>high</td>\n      <td>easy</td>\n      <td>100.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"data.dtypes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T10:45:58.750076Z","iopub.execute_input":"2026-01-18T10:45:58.750509Z","iopub.status.idle":"2026-01-18T10:45:58.759938Z","shell.execute_reply.started":"2026-01-18T10:45:58.750470Z","shell.execute_reply":"2026-01-18T10:45:58.758632Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"id                    int64\nage                   int64\ngender               object\ncourse               object\nstudy_hours         float64\nclass_attendance    float64\ninternet_access      object\nsleep_hours         float64\nsleep_quality        object\nstudy_method         object\nfacility_rating      object\nexam_difficulty      object\nexam_score          float64\ndtype: object"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"col = data.columns\ncol","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T10:45:58.761417Z","iopub.execute_input":"2026-01-18T10:45:58.761799Z","iopub.status.idle":"2026-01-18T10:45:58.780517Z","shell.execute_reply.started":"2026-01-18T10:45:58.761764Z","shell.execute_reply":"2026-01-18T10:45:58.779554Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Index(['id', 'age', 'gender', 'course', 'study_hours', 'class_attendance',\n       'internet_access', 'sleep_hours', 'sleep_quality', 'study_method',\n       'facility_rating', 'exam_difficulty', 'exam_score'],\n      dtype='object')"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"def missing(col):\n    mis = {}\n    for i in col:\n        mis[i] = data[i].isna().sum()\n    return mis","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T10:45:58.781604Z","iopub.execute_input":"2026-01-18T10:45:58.781863Z","iopub.status.idle":"2026-01-18T10:45:58.797482Z","shell.execute_reply.started":"2026-01-18T10:45:58.781829Z","shell.execute_reply":"2026-01-18T10:45:58.796394Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"print(missing(col))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T10:45:58.798807Z","iopub.execute_input":"2026-01-18T10:45:58.799774Z","iopub.status.idle":"2026-01-18T10:45:59.039206Z","shell.execute_reply.started":"2026-01-18T10:45:58.799729Z","shell.execute_reply":"2026-01-18T10:45:59.037944Z"}},"outputs":[{"name":"stdout","text":"{'id': np.int64(0), 'age': np.int64(0), 'gender': np.int64(0), 'course': np.int64(0), 'study_hours': np.int64(0), 'class_attendance': np.int64(0), 'internet_access': np.int64(0), 'sleep_hours': np.int64(0), 'sleep_quality': np.int64(0), 'study_method': np.int64(0), 'facility_rating': np.int64(0), 'exam_difficulty': np.int64(0), 'exam_score': np.int64(0)}\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def obj(col):\n    ob = []\n    for i in col:\n        if data[i].dtype == \"object\":\n            ob.append(i)\n        else:\n            pass\n    return ob","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T10:45:59.042672Z","iopub.execute_input":"2026-01-18T10:45:59.042989Z","iopub.status.idle":"2026-01-18T10:45:59.049171Z","shell.execute_reply.started":"2026-01-18T10:45:59.042962Z","shell.execute_reply":"2026-01-18T10:45:59.048022Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"o1 = obj(col)\no1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T10:45:59.050596Z","iopub.execute_input":"2026-01-18T10:45:59.050996Z","iopub.status.idle":"2026-01-18T10:45:59.072907Z","shell.execute_reply.started":"2026-01-18T10:45:59.050958Z","shell.execute_reply":"2026-01-18T10:45:59.071658Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"['gender',\n 'course',\n 'internet_access',\n 'sleep_quality',\n 'study_method',\n 'facility_rating',\n 'exam_difficulty']"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T10:45:59.074359Z","iopub.execute_input":"2026-01-18T10:45:59.074729Z","iopub.status.idle":"2026-01-18T10:46:00.907173Z","shell.execute_reply.started":"2026-01-18T10:45:59.074692Z","shell.execute_reply":"2026-01-18T10:46:00.906064Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"one = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\nTransform = ColumnTransformer(transformers = [(\"one\", one, o1)] , remainder = \"passthrough\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T10:46:00.908543Z","iopub.execute_input":"2026-01-18T10:46:00.909054Z","iopub.status.idle":"2026-01-18T10:46:00.915281Z","shell.execute_reply.started":"2026-01-18T10:46:00.909012Z","shell.execute_reply":"2026-01-18T10:46:00.914288Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"y = data[\"exam_score\"]\nx = data.drop(columns=[\"exam_score\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T10:46:00.916666Z","iopub.execute_input":"2026-01-18T10:46:00.917431Z","iopub.status.idle":"2026-01-18T10:46:01.042684Z","shell.execute_reply.started":"2026-01-18T10:46:00.917391Z","shell.execute_reply":"2026-01-18T10:46:01.041426Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"len(x) , len(y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T10:46:01.043986Z","iopub.execute_input":"2026-01-18T10:46:01.044291Z","iopub.status.idle":"2026-01-18T10:46:01.052541Z","shell.execute_reply.started":"2026-01-18T10:46:01.044263Z","shell.execute_reply":"2026-01-18T10:46:01.051372Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(630000, 630000)"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"t_x = Transform.fit_transform(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T10:46:01.053925Z","iopub.execute_input":"2026-01-18T10:46:01.054298Z","iopub.status.idle":"2026-01-18T10:46:02.557967Z","shell.execute_reply.started":"2026-01-18T10:46:01.054260Z","shell.execute_reply":"2026-01-18T10:46:02.556719Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T10:46:02.559571Z","iopub.execute_input":"2026-01-18T10:46:02.559956Z","iopub.status.idle":"2026-01-18T10:46:02.565665Z","shell.execute_reply.started":"2026-01-18T10:46:02.559917Z","shell.execute_reply":"2026-01-18T10:46:02.564264Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"x_train , x_test , y_train , y_test =  train_test_split(t_x , y , test_size = 0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T10:46:02.566963Z","iopub.execute_input":"2026-01-18T10:46:02.567372Z","iopub.status.idle":"2026-01-18T10:46:02.703774Z","shell.execute_reply.started":"2026-01-18T10:46:02.567311Z","shell.execute_reply":"2026-01-18T10:46:02.702661Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"len(x_train) , len(x_test) , len(y_train) , len(y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T10:46:02.705034Z","iopub.execute_input":"2026-01-18T10:46:02.705443Z","iopub.status.idle":"2026-01-18T10:46:02.713369Z","shell.execute_reply.started":"2026-01-18T10:46:02.705391Z","shell.execute_reply":"2026-01-18T10:46:02.712182Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"(504000, 126000, 504000, 126000)"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"from lightgbm import LGBMRegressor\nm1 = LGBMRegressor(objective = \"regression\" , random_state = 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T10:46:02.714745Z","iopub.execute_input":"2026-01-18T10:46:02.715046Z","iopub.status.idle":"2026-01-18T10:46:09.856827Z","shell.execute_reply.started":"2026-01-18T10:46:02.715020Z","shell.execute_reply":"2026-01-18T10:46:09.855837Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"grid = {\"n_estimators\": [500, 1000, 2000],\n    \"learning_rate\": [0.05, 0.03, 0.01],\n    \"num_leaves\": [31, 63, 127],\n    \"max_depth\": [-1, 10, 20],\n    \"min_child_samples\": [20, 40, 60],\n    \"subsample\": [0.7, 0.8, 0.9],\n    \"colsample_bytree\": [0.7, 0.8, 0.9],\n    \"reg_alpha\": [0.0, 0.1, 0.5],\n    \"reg_lambda\": [0.0, 0.1, 0.5]}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T10:46:09.857869Z","iopub.execute_input":"2026-01-18T10:46:09.858741Z","iopub.status.idle":"2026-01-18T10:46:09.865001Z","shell.execute_reply.started":"2026-01-18T10:46:09.858703Z","shell.execute_reply":"2026-01-18T10:46:09.864021Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from sklearn.model_selection import KFold","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T10:46:09.866414Z","iopub.execute_input":"2026-01-18T10:46:09.866740Z","iopub.status.idle":"2026-01-18T10:46:09.984895Z","shell.execute_reply.started":"2026-01-18T10:46:09.866714Z","shell.execute_reply":"2026-01-18T10:46:09.983840Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\nbest = RandomizedSearchCV(\n    estimator=m1,\n    param_distributions=grid,\n    n_iter=20,                      \n    scoring=\"neg_root_mean_squared_error\",\n    cv=3,\n    n_jobs=-1,\n    random_state=42,\n    verbose=2\n)\n\nbest.fit(x_train, y_train)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T10:46:09.986128Z","iopub.execute_input":"2026-01-18T10:46:09.986540Z","iopub.status.idle":"2026-01-18T11:26:04.856781Z","shell.execute_reply.started":"2026-01-18T10:46:09.986503Z","shell.execute_reply":"2026-01-18T11:26:04.855592Z"}},"outputs":[{"name":"stdout","text":"Fitting 3 folds for each of 20 candidates, totalling 60 fits\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.089730 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.517523\n[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=10, min_child_samples=20, n_estimators=1000, num_leaves=63, reg_alpha=0.5, reg_lambda=0.1, subsample=0.9; total time= 1.6min\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117911 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.535919\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=10, min_child_samples=40, n_estimators=500, num_leaves=63, reg_alpha=0.1, reg_lambda=0.5, subsample=0.9; total time= 2.1min\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043330 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.524725\n[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=10, min_child_samples=40, n_estimators=500, num_leaves=127, reg_alpha=0.0, reg_lambda=0.1, subsample=0.7; total time= 2.2min\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051933 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.524725\n[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, reg_alpha=0.5, reg_lambda=0.5, subsample=0.8; total time= 2.6min\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063336 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.517523\n[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=20, min_child_samples=20, n_estimators=1000, num_leaves=31, reg_alpha=0.0, reg_lambda=0.0, subsample=0.9; total time= 1.5min\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.085737 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.535919\n[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=20, min_child_samples=20, n_estimators=1000, num_leaves=31, reg_alpha=0.0, reg_lambda=0.0, subsample=0.9; total time= 2.0min\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.102975 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.517523\n[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=10, min_child_samples=60, n_estimators=1000, num_leaves=31, reg_alpha=0.0, reg_lambda=0.1, subsample=0.9; total time= 2.2min\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070012 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.524725\n[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=10, min_child_samples=60, n_estimators=1000, num_leaves=31, reg_alpha=0.0, reg_lambda=0.1, subsample=0.9; total time= 1.5min\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048791 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.524725\n[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=-1, min_child_samples=40, n_estimators=2000, num_leaves=31, reg_alpha=0.5, reg_lambda=0.1, subsample=0.9; total time= 3.9min\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068231 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.517523\n[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=20, min_child_samples=40, n_estimators=500, num_leaves=127, reg_alpha=0.0, reg_lambda=0.5, subsample=0.7; total time= 1.1min\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088846 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.535919\n[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=20, min_child_samples=40, n_estimators=500, num_leaves=127, reg_alpha=0.0, reg_lambda=0.5, subsample=0.7; total time= 1.3min\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037070 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.517523\n[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=20, min_child_samples=20, n_estimators=2000, num_leaves=127, reg_alpha=0.1, reg_lambda=0.0, subsample=0.9; total time= 4.0min\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057829 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.535919\n[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, reg_alpha=0.0, reg_lambda=0.1, subsample=0.7; total time= 1.1min\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046748 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.524725\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=10, min_child_samples=20, n_estimators=2000, num_leaves=63, reg_alpha=0.5, reg_lambda=0.0, subsample=0.8; total time= 4.2min\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.085257 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.127960 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.535919\n[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=-1, min_child_samples=60, n_estimators=500, num_leaves=31, reg_alpha=0.0, reg_lambda=0.0, subsample=0.7; total time= 1.1min\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.101724 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.524725\n[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=10, min_child_samples=20, n_estimators=1000, num_leaves=63, reg_alpha=0.5, reg_lambda=0.1, subsample=0.9; total time= 2.3min\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.101975 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.535919\n[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=10, min_child_samples=40, n_estimators=500, num_leaves=127, reg_alpha=0.0, reg_lambda=0.1, subsample=0.7; total time= 2.6min\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068108 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.517523\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=20, min_child_samples=40, n_estimators=2000, num_leaves=63, reg_alpha=0.0, reg_lambda=0.0, subsample=0.8; total time= 4.0min\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087010 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.524725\n[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=20, min_child_samples=20, n_estimators=1000, num_leaves=31, reg_alpha=0.0, reg_lambda=0.0, subsample=0.9; total time= 1.8min\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076649 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.535919\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, reg_alpha=0.5, reg_lambda=0.5, subsample=0.8; total time= 2.4min\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.108832 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.517523\n[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=-1, min_child_samples=40, n_estimators=2000, num_leaves=31, reg_alpha=0.5, reg_lambda=0.1, subsample=0.9; total time= 4.7min\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047409 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.524725\n[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=-1, min_child_samples=60, n_estimators=2000, num_leaves=127, reg_alpha=0.1, reg_lambda=0.1, subsample=0.7; total time= 4.5min\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.096219 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.517523\n[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=10, min_child_samples=20, n_estimators=500, num_leaves=63, reg_alpha=0.1, reg_lambda=0.1, subsample=0.8; total time= 1.1min\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057743 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.535919\n[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=10, min_child_samples=20, n_estimators=500, num_leaves=63, reg_alpha=0.1, reg_lambda=0.1, subsample=0.8; total time=  56.0s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.139953 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.524725\n[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=10, min_child_samples=20, n_estimators=500, num_leaves=63, reg_alpha=0.1, reg_lambda=0.1, subsample=0.8; total time= 1.5min\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048364 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.535919\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=10, min_child_samples=20, n_estimators=2000, num_leaves=63, reg_alpha=0.5, reg_lambda=0.0, subsample=0.8; total time= 5.4min\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051397 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.535919\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=20, min_child_samples=60, n_estimators=1000, num_leaves=31, reg_alpha=0.5, reg_lambda=0.1, subsample=0.7; total time= 2.0min\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082586 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.524725\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=20, min_child_samples=60, n_estimators=1000, num_leaves=31, reg_alpha=0.5, reg_lambda=0.1, subsample=0.7; total time= 2.6min\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.096665 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053515 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.517523\n[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=-1, min_child_samples=60, n_estimators=500, num_leaves=31, reg_alpha=0.0, reg_lambda=0.0, subsample=0.7; total time= 1.0min\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057638 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.535919\n[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=10, min_child_samples=20, n_estimators=1000, num_leaves=63, reg_alpha=0.5, reg_lambda=0.1, subsample=0.9; total time= 1.5min\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052959 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.524725\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=10, min_child_samples=40, n_estimators=500, num_leaves=63, reg_alpha=0.1, reg_lambda=0.5, subsample=0.9; total time= 1.5min\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061266 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.517523\n[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, reg_alpha=0.5, reg_lambda=0.5, subsample=0.8; total time= 2.3min\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.094339 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.535919\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=20, min_child_samples=40, n_estimators=2000, num_leaves=63, reg_alpha=0.0, reg_lambda=0.0, subsample=0.8; total time= 5.0min\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.120725 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.517523\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, reg_alpha=0.5, reg_lambda=0.5, subsample=0.8; total time= 2.6min\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.104966 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.535919\n[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=10, min_child_samples=60, n_estimators=1000, num_leaves=31, reg_alpha=0.0, reg_lambda=0.1, subsample=0.9; total time= 2.3min\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.089157 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.517523\n[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=-1, min_child_samples=60, n_estimators=2000, num_leaves=127, reg_alpha=0.1, reg_lambda=0.1, subsample=0.7; total time= 5.0min\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045622 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.524725\n[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=20, min_child_samples=40, n_estimators=500, num_leaves=127, reg_alpha=0.0, reg_lambda=0.5, subsample=0.7; total time= 1.2min\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116489 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.535919\n[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=20, min_child_samples=20, n_estimators=2000, num_leaves=127, reg_alpha=0.1, reg_lambda=0.0, subsample=0.9; total time= 4.0min\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060780 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.524725\n[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, reg_alpha=0.0, reg_lambda=0.1, subsample=0.7; total time= 1.5min\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071053 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.517523\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=-1, min_child_samples=60, n_estimators=2000, num_leaves=63, reg_alpha=0.0, reg_lambda=0.0, subsample=0.9; total time= 4.0min\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.113098 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.517523\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=20, min_child_samples=60, n_estimators=1000, num_leaves=31, reg_alpha=0.5, reg_lambda=0.1, subsample=0.7; total time= 3.0min\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116525 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.517523\n[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, reg_alpha=0.5, reg_lambda=0.0, subsample=0.7; total time= 2.2min\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.090967 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 336000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.535919","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020086 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 876\n[LightGBM] [Info] Number of data points in the train set: 504000, number of used features: 31\n[LightGBM] [Info] Start training from score 62.526056\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"RandomizedSearchCV(cv=3,\n                   estimator=LGBMRegressor(objective='regression',\n                                           random_state=1),\n                   n_iter=20, n_jobs=-1,\n                   param_distributions={'colsample_bytree': [0.7, 0.8, 0.9],\n                                        'learning_rate': [0.05, 0.03, 0.01],\n                                        'max_depth': [-1, 10, 20],\n                                        'min_child_samples': [20, 40, 60],\n                                        'n_estimators': [500, 1000, 2000],\n                                        'num_leaves': [31, 63, 127],\n                                        'reg_alpha': [0.0, 0.1, 0.5],\n                                        'reg_lambda': [0.0, 0.1, 0.5],\n                                        'subsample': [0.7, 0.8, 0.9]},\n                   random_state=42, scoring='neg_root_mean_squared_error',\n                   verbose=2)","text/html":"<style>#sk-container-id-1 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: #000;\n  --sklearn-color-text-muted: #666;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-1 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-1 pre {\n  padding: 0;\n}\n\n#sk-container-id-1 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-1 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-1 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-1 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-1 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-1 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-1 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-1 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-1 label.sk-toggleable__label {\n  cursor: pointer;\n  display: flex;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n  align-items: start;\n  justify-content: space-between;\n  gap: 0.5em;\n}\n\n#sk-container-id-1 label.sk-toggleable__label .caption {\n  font-size: 0.6rem;\n  font-weight: lighter;\n  color: var(--sklearn-color-text-muted);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-1 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n#sk-container-id-1 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-1 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-1 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-1 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-1 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 0.5em;\n  text-align: center;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-1 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-1 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3,\n                   estimator=LGBMRegressor(objective=&#x27;regression&#x27;,\n                                           random_state=1),\n                   n_iter=20, n_jobs=-1,\n                   param_distributions={&#x27;colsample_bytree&#x27;: [0.7, 0.8, 0.9],\n                                        &#x27;learning_rate&#x27;: [0.05, 0.03, 0.01],\n                                        &#x27;max_depth&#x27;: [-1, 10, 20],\n                                        &#x27;min_child_samples&#x27;: [20, 40, 60],\n                                        &#x27;n_estimators&#x27;: [500, 1000, 2000],\n                                        &#x27;num_leaves&#x27;: [31, 63, 127],\n                                        &#x27;reg_alpha&#x27;: [0.0, 0.1, 0.5],\n                                        &#x27;reg_lambda&#x27;: [0.0, 0.1, 0.5],\n                                        &#x27;subsample&#x27;: [0.7, 0.8, 0.9]},\n                   random_state=42, scoring=&#x27;neg_root_mean_squared_error&#x27;,\n                   verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RandomizedSearchCV</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\">?<span>Documentation for RandomizedSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>RandomizedSearchCV(cv=3,\n                   estimator=LGBMRegressor(objective=&#x27;regression&#x27;,\n                                           random_state=1),\n                   n_iter=20, n_jobs=-1,\n                   param_distributions={&#x27;colsample_bytree&#x27;: [0.7, 0.8, 0.9],\n                                        &#x27;learning_rate&#x27;: [0.05, 0.03, 0.01],\n                                        &#x27;max_depth&#x27;: [-1, 10, 20],\n                                        &#x27;min_child_samples&#x27;: [20, 40, 60],\n                                        &#x27;n_estimators&#x27;: [500, 1000, 2000],\n                                        &#x27;num_leaves&#x27;: [31, 63, 127],\n                                        &#x27;reg_alpha&#x27;: [0.0, 0.1, 0.5],\n                                        &#x27;reg_lambda&#x27;: [0.0, 0.1, 0.5],\n                                        &#x27;subsample&#x27;: [0.7, 0.8, 0.9]},\n                   random_state=42, scoring=&#x27;neg_root_mean_squared_error&#x27;,\n                   verbose=2)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>best_estimator_: LGBMRegressor</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>LGBMRegressor(colsample_bytree=0.7, learning_rate=0.01, min_child_samples=60,\n              n_estimators=2000, num_leaves=63, objective=&#x27;regression&#x27;,\n              random_state=1, subsample=0.9)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LGBMRegressor</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>LGBMRegressor(colsample_bytree=0.7, learning_rate=0.01, min_child_samples=60,\n              n_estimators=2000, num_leaves=63, objective=&#x27;regression&#x27;,\n              random_state=1, subsample=0.9)</pre></div> </div></div></div></div></div></div></div></div></div>"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\ny_val_pred = best.predict(x_test)\nrmse = mean_squared_error(y_test, y_val_pred)\nrmse = rmse ** 0.5\nprint(rmse)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T11:47:07.074152Z","iopub.execute_input":"2026-01-18T11:47:07.074522Z","iopub.status.idle":"2026-01-18T11:47:26.423216Z","shell.execute_reply.started":"2026-01-18T11:47:07.074494Z","shell.execute_reply":"2026-01-18T11:47:26.422297Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"8.767070801399814\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/playground-test/test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T11:48:09.798305Z","iopub.execute_input":"2026-01-18T11:48:09.798902Z","iopub.status.idle":"2026-01-18T11:48:10.123238Z","shell.execute_reply.started":"2026-01-18T11:48:09.798870Z","shell.execute_reply":"2026-01-18T11:48:10.122428Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T11:48:11.104309Z","iopub.execute_input":"2026-01-18T11:48:11.105298Z","iopub.status.idle":"2026-01-18T11:48:11.123796Z","shell.execute_reply.started":"2026-01-18T11:48:11.105264Z","shell.execute_reply":"2026-01-18T11:48:11.122793Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"            id  age  gender   course  study_hours  class_attendance  \\\n0       630000   24   other       ba         6.85              65.2   \n1       630001   18    male  diploma         6.61              45.0   \n2       630002   24  female   b.tech         6.60              98.5   \n3       630003   24    male  diploma         3.03              66.3   \n4       630004   20  female   b.tech         2.03              42.4   \n...        ...  ...     ...      ...          ...               ...   \n269995  899995   21   other    b.com         2.55              82.3   \n269996  899996   17  female    b.com         0.49              46.4   \n269997  899997   22    male      bba         6.62              74.7   \n269998  899998   22   other       ba         4.08              51.8   \n269999  899999   20  female    b.com         5.86              59.7   \n\n       internet_access  sleep_hours sleep_quality   study_method  \\\n0                  yes          5.2          poor    group study   \n1                   no          9.3          poor       coaching   \n2                  yes          6.2          good    group study   \n3                  yes          5.7       average          mixed   \n4                  yes          9.2       average       coaching   \n...                ...          ...           ...            ...   \n269995             yes          8.4       average          mixed   \n269996             yes          8.8          good          mixed   \n269997             yes          5.5          good       coaching   \n269998             yes          8.7          poor  online videos   \n269999             yes          8.9          poor          mixed   \n\n       facility_rating exam_difficulty  \n0                 high            easy  \n1                  low            easy  \n2               medium        moderate  \n3               medium        moderate  \n4                  low        moderate  \n...                ...             ...  \n269995          medium            hard  \n269996             low            easy  \n269997            high            easy  \n269998            high        moderate  \n269999          medium        moderate  \n\n[270000 rows x 12 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>age</th>\n      <th>gender</th>\n      <th>course</th>\n      <th>study_hours</th>\n      <th>class_attendance</th>\n      <th>internet_access</th>\n      <th>sleep_hours</th>\n      <th>sleep_quality</th>\n      <th>study_method</th>\n      <th>facility_rating</th>\n      <th>exam_difficulty</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>630000</td>\n      <td>24</td>\n      <td>other</td>\n      <td>ba</td>\n      <td>6.85</td>\n      <td>65.2</td>\n      <td>yes</td>\n      <td>5.2</td>\n      <td>poor</td>\n      <td>group study</td>\n      <td>high</td>\n      <td>easy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>630001</td>\n      <td>18</td>\n      <td>male</td>\n      <td>diploma</td>\n      <td>6.61</td>\n      <td>45.0</td>\n      <td>no</td>\n      <td>9.3</td>\n      <td>poor</td>\n      <td>coaching</td>\n      <td>low</td>\n      <td>easy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>630002</td>\n      <td>24</td>\n      <td>female</td>\n      <td>b.tech</td>\n      <td>6.60</td>\n      <td>98.5</td>\n      <td>yes</td>\n      <td>6.2</td>\n      <td>good</td>\n      <td>group study</td>\n      <td>medium</td>\n      <td>moderate</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>630003</td>\n      <td>24</td>\n      <td>male</td>\n      <td>diploma</td>\n      <td>3.03</td>\n      <td>66.3</td>\n      <td>yes</td>\n      <td>5.7</td>\n      <td>average</td>\n      <td>mixed</td>\n      <td>medium</td>\n      <td>moderate</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>630004</td>\n      <td>20</td>\n      <td>female</td>\n      <td>b.tech</td>\n      <td>2.03</td>\n      <td>42.4</td>\n      <td>yes</td>\n      <td>9.2</td>\n      <td>average</td>\n      <td>coaching</td>\n      <td>low</td>\n      <td>moderate</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>269995</th>\n      <td>899995</td>\n      <td>21</td>\n      <td>other</td>\n      <td>b.com</td>\n      <td>2.55</td>\n      <td>82.3</td>\n      <td>yes</td>\n      <td>8.4</td>\n      <td>average</td>\n      <td>mixed</td>\n      <td>medium</td>\n      <td>hard</td>\n    </tr>\n    <tr>\n      <th>269996</th>\n      <td>899996</td>\n      <td>17</td>\n      <td>female</td>\n      <td>b.com</td>\n      <td>0.49</td>\n      <td>46.4</td>\n      <td>yes</td>\n      <td>8.8</td>\n      <td>good</td>\n      <td>mixed</td>\n      <td>low</td>\n      <td>easy</td>\n    </tr>\n    <tr>\n      <th>269997</th>\n      <td>899997</td>\n      <td>22</td>\n      <td>male</td>\n      <td>bba</td>\n      <td>6.62</td>\n      <td>74.7</td>\n      <td>yes</td>\n      <td>5.5</td>\n      <td>good</td>\n      <td>coaching</td>\n      <td>high</td>\n      <td>easy</td>\n    </tr>\n    <tr>\n      <th>269998</th>\n      <td>899998</td>\n      <td>22</td>\n      <td>other</td>\n      <td>ba</td>\n      <td>4.08</td>\n      <td>51.8</td>\n      <td>yes</td>\n      <td>8.7</td>\n      <td>poor</td>\n      <td>online videos</td>\n      <td>high</td>\n      <td>moderate</td>\n    </tr>\n    <tr>\n      <th>269999</th>\n      <td>899999</td>\n      <td>20</td>\n      <td>female</td>\n      <td>b.com</td>\n      <td>5.86</td>\n      <td>59.7</td>\n      <td>yes</td>\n      <td>8.9</td>\n      <td>poor</td>\n      <td>mixed</td>\n      <td>medium</td>\n      <td>moderate</td>\n    </tr>\n  </tbody>\n</table>\n<p>270000 rows  12 columns</p>\n</div>"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"test.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T11:48:14.613239Z","iopub.execute_input":"2026-01-18T11:48:14.614112Z","iopub.status.idle":"2026-01-18T11:48:14.620166Z","shell.execute_reply.started":"2026-01-18T11:48:14.614079Z","shell.execute_reply":"2026-01-18T11:48:14.619223Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"Index(['id', 'age', 'gender', 'course', 'study_hours', 'class_attendance',\n       'internet_access', 'sleep_hours', 'sleep_quality', 'study_method',\n       'facility_rating', 'exam_difficulty'],\n      dtype='object')"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"t_test = Transform.fit_transform(test)\npreds = best.predict(t_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T11:48:18.093200Z","iopub.execute_input":"2026-01-18T11:48:18.094270Z","iopub.status.idle":"2026-01-18T11:49:00.480471Z","shell.execute_reply.started":"2026-01-18T11:48:18.094229Z","shell.execute_reply":"2026-01-18T11:49:00.479433Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"preds","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = pd.DataFrame({\"id\": test[\"id\"], \"exam_score\": preds}) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission.to_csv(\"2nd_submission.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}